# config_a30_hybrid.py
from dataclasses import dataclass
import torch


@dataclass
class A30ConfigHybrid:
    """
    ============================================================
    A30 Hybrid Training Config
    ============================================================

    设计目标：
    - 同一套代码：
        * CPU 能稳定长跑（PSRO / CE）
        * GPU 能高吞吐训练（Self-Play / PSRO 后期）
    - 参数全部为「经验 + 实测」而非随意拍脑袋

    适用任务：
    - 德州扑克 / 炸金花
    - Counter-Exploit（CE-1 / CE-2 / CE-3）
    - PSRO-1 / PSRO-2 / PSRO-3

    ⚠️ 重要原则：
    - 标注为【结构性参数】的，修改后必须全部重训
    - 标注为【稳定性参数】的，影响是否“卡死 / 爆炸 / 振荡”
    ============================================================
    """

    # ============================================================
    # 一、环境 & 网络结构（【结构性参数】）
    # ============================================================

    # 环境返回给 agent 的状态向量维度
    # 对应：ZJHEnvMulti14_8p._get_state()
    #
    # ⚠️ 修改它 = 网络输入层改变 = 所有历史模型全部失效
    state_dim: int = 49

    # 动作空间大小
    # a00 ~ a13，一共 14 个动作
    #
    # ⚠️ 改它 = policy head 结构变化 = 必须重训
    action_dim: int = 14

    # 折扣因子 γ
    #
    # 含义：
    # - 越接近 1 → 越重视「整局 / 多轮」长期 EV
    # - 非常适合：
    #     * 扑克
    #     * 多轮下注
    #     * Exploit / Counter-Exploit
    #
    # 实战经验：
    # - 0.99：稳定、偏 GTO / PSRO
    # - 0.95：短期 aggressive 行为变多，EV 方差明显上升
    gamma: float = 0.99

    # ============================================================
    # 二、训练规模（【结构性参数】）
    # ============================================================

    # 总训练局数
    # PSRO / CE 都是「慢热型」，50 万是经验下限
    total_episodes: int = 500_000

    # 每一局最多 env.step 次数
    #
    # 作用：
    # - 防止对手死循环
    # - 防止异常状态卡死
    #
    # 对 8 人炸金花来说：
    # - 正常一局：< 100 step
    # - 200 是「绝对安全上限」
    max_steps_per_episode: int = 200

    # ============================================================
    # 三、设备自适应（CPU / GPU）
    # ============================================================

    # 主计算设备
    device: str = "cuda" if torch.cuda.is_available() else "cpu"
    has_gpu: bool = torch.cuda.is_available()

    # ============================================================
    # 四、Batch / 学习频率（【稳定性参数】）
    # ============================================================

    # batch_size
    #
    # GPU：
    # - 1024：高吞吐、低梯度噪声
    #
    # CPU：
    # - 256：防止反向传播拖垮系统
    batch_size: int = 1024 if torch.cuda.is_available() else 256

    # 每多少 step 进行一次 learn
    #
    # GPU：
    # - 每 step 学一次，最大化吞吐
    #
    # CPU：
    # - 每 2 step 学一次，减少反向传播压力
    learn_interval: int = 1 if torch.cuda.is_available() else 2

    # ============================================================
    # 五、优化器参数（【稳定性参数】）
    # ============================================================

    # 学习率
    #
    # GPU：
    # - batch 大、梯度稳定 → lr 可稍大
    #
    # CPU：
    # - batch 小 → lr 必须保守
    lr: float = 1e-4 if torch.cuda.is_available() else 6e-5

    # 权重衰减
    # 对 DQN 系列基本没收益，保持 0
    weight_decay: float = 0.0

    # ============================================================
    # 六、Replay Buffer（【稳定性 + 多策略记忆】）
    # ============================================================

    # Replay Buffer 容量
    #
    # GPU：
    # - 600k：能覆盖多个 PSRO 阶段
    #
    # CPU：
    # - 300k：内存 + 速度平衡
    #
    # 目的：
    # - 防止新策略过快覆盖旧策略
    # - 保留 exploit 历史
    buffer_capacity: int = 600_000 if torch.cuda.is_available() else 300_000

    # ============================================================
    # 七、PER（Prioritized Experience Replay）
    # ============================================================

    # TD-error 权重系数
    # 0.6 是 DQN / Rainbow 系列的经典值
    per_alpha: float = 0.6

    # importance-sampling beta
    #
    # 从「有偏探索」→「无偏收敛」
    # 400k steps 正好覆盖：
    # - Self-Play
    # - PSRO 中后期
    per_beta_start: float = 0.4
    per_beta_end: float = 1.0
    per_beta_anneal_steps: int = 400_000

    # ============================================================
    # 八、Target Network 更新
    # ============================================================

    # target 网络同步频率
    #
    # GPU：
    # - 更新快，收敛速度高
    #
    # CPU：
    # - 更新慢，避免 value 震荡
    target_update_freq: int = 1000 if torch.cuda.is_available() else 2000

    # ============================================================
    # 九、epsilon（主要用于日志 & 分析）
    # ============================================================

    # 实际训练中：
    # - Exploit / PSRO 多数用固定 eps（如 0.01~0.025）
    #
    # 这里主要用于：
    # - 日志
    # - 可视化
    eps_start: float = 0.05
    eps_end: float = 0.01
    eps_decay_episodes: int = 400_000

    # ============================================================
    # 十、梯度稳定性
    # ============================================================

    # 梯度裁剪
    #
    # GPU：
    # - 梯度更稳定 → 裁剪更严格
    #
    # CPU：
    # - batch 小 → 允许更大裁剪
    max_grad_norm: float = 5.0 if torch.cuda.is_available() else 10.0

    # 梯度累积
    #
    # GPU：
    # - 2 step 累积 ≈ 更大 batch
    #
    # CPU：
    # - 关闭，避免延迟
    grad_accumulate_steps: int = 2 if torch.cuda.is_available() else 1

    # ============================================================
    # 十一、日志 & 评估频率
    # ============================================================

    # 训练日志频率
    log_interval: int = 5000

    # PSRO / CE 评估频率
    eval_interval: int = 20000

    # ============================================================
    # 十二、随机性控制
    # ============================================================

    # 全局随机种子
    seed: int = 42

    # ============================================================
    # 十三、Population（PSRO 专用）
    # ============================================================

    # Population 准入阈值
    #
    # score = worstEV + α * avgEV
    #
    # 为什么是 -25？
    # - 实测：
    #     worstEV 常在 -30 ~ -40
    # - 但这些策略：
    #     * 仍能 exploit 特定对手
    #     * 对 PSRO 多样性非常重要
    #
    # 建议策略：
    # - PSRO-1 / 2：-25
    # - PSRO-3 后期：逐步收紧到 -15 / -10
    min_pop_score: float = -15.0

    # 每次评估的对局数
    eval_episodes: int = 5_000

    # 并行评估环境数
    eval_envs: int = 32
